{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook Data Crawling\n",
    "\n",
    "In this notebook, we will be crawling data from Facebook using the Facebook Graph API. We will be using the facebook-scraper\n",
    "\n",
    "Install the required library\n",
    "\n",
    "We will be using the facebook-scraper library to crawl data from Facebook. We will install this library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facebook_scraper in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.59)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: dateparser<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from facebook_scraper) (1.2.0)\n",
      "Requirement already satisfied: demjson3<4.0.0,>=3.0.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from facebook_scraper) (3.0.6)\n",
      "Requirement already satisfied: requests-html<0.11.0,>=0.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from facebook_scraper) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (2023.10.3)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (5.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.31.0)\n",
      "Requirement already satisfied: pyquery in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.4.0)\n",
      "Requirement already satisfied: parse in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.20.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (0.0.1)\n",
      "Requirement already satisfied: w3lib in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.1.2)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.0.2)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2023.11.17)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (7.0.0)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (8.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.66.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.26.18)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.12.2)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.9.3)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install facebook_scraper pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl the data using facebook_scraper\n",
    "Now we can get the data from Facebook using the facebook_scraper library. We will be using the get_posts function to get the posts from the fanpage. This function will return a list of dictionaries, where each dictionary represents a post. We will be saving this list of dictionaries to a json file. More information about what you can do with the facebook_scraper library can be found here: https://github.com/kevinzg/facebook-scraper\n",
    "\n",
    "Define variables\n",
    "First we have to define some variables that we will be using throughout the notebook.\n",
    "\n",
    "FANPAGE_LINK: The link to the fanpage that we want to crawl data from. This can be found by going to the fanpage and copying the link from the address bar. For example, the link to the fanpage of the Nintendo Switch is https://www.facebook.com/NintendoSwitch/. We will be using this link as the value for FANPAGE_LINK.\n",
    "\n",
    "COOKIE_PATH: The path to the cookie file that we will be using to authenticate with Facebook. This cookie file can be obtained by logging into Facebook and copying the cookie from the browser. For example, in Chromium, use extension Get cookies.txt LOCALLY to get the cookie file. Then save the cookie to a file and use the path to this file as the value for COOKIE_PATH. USE COOKIE FROM A FAKE ACCOUNT, OTHERWISE YOUR REAL ACCOUNT MIGHT GET BANNED..\n",
    "\n",
    "FOLDER_NAME: The name of the folder that we will be saving the data to. This folder will be created in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FANPAGE_LINK = \"HonkaiStarRail.VN\"\n",
    "FOLDER_PATH = \"C:/Final project/Data/\"\n",
    "COOKIE_PATH = \"C:/Final project/cookies.txt\"\n",
    "\n",
    "PAGES_NUMBER = 10 # Number of pages to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedResponse",
     "evalue": "Your request couldn't be processed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedResponse\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Final project\\Crawl_Data.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m post_list \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m post \u001b[39min\u001b[39;49;00m get_posts(FANPAGE_LINK,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcomments\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mreactions\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mallow_extra_requests\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                     extra_info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, pages\u001b[39m=\u001b[39;49mPAGES_NUMBER, cookies\u001b[39m=\u001b[39;49mCOOKIE_PATH):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(post)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Final%20project/Crawl_Data.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     post_list\u001b[39m.\u001b[39;49mappend(post)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facebook_scraper\\facebook_scraper.py:1114\u001b[0m, in \u001b[0;36mFacebookScraper._generic_get_posts\u001b[1;34m(self, extract_post_fn, iter_pages_fn, page_limit, options, remove_source, latest_date, max_past_limit, **kwargs)\u001b[0m\n\u001b[0;32m   1111\u001b[0m counter \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mcount(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m page_limit \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mrange\u001b[39m(page_limit)\n\u001b[0;32m   1113\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mStarting to iterate pages\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1114\u001b[0m \u001b[39mfor\u001b[39;49;00m i, page \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(counter, iter_pages_fn()):\n\u001b[0;32m   1115\u001b[0m     logger\u001b[39m.\u001b[39;49mdebug(\u001b[39m\"\u001b[39;49m\u001b[39mExtracting posts from page \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, i)\n\u001b[0;32m   1116\u001b[0m     \u001b[39mfor\u001b[39;49;00m post_element \u001b[39min\u001b[39;49;00m page:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facebook_scraper\\page_iterators.py:87\u001b[0m, in \u001b[0;36mgeneric_iter_pages\u001b[1;34m(start_url, page_parser_cls, request_fn, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRequesting page from: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, next_url)\n\u001b[1;32m---> 87\u001b[0m     response \u001b[39m=\u001b[39m request_fn(next_url)\n\u001b[0;32m     88\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\facebook_scraper\\facebook_scraper.py:927\u001b[0m, in \u001b[0;36mFacebookScraper.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mNotFound(title\u001b[39m.\u001b[39mtext)\n\u001b[0;32m    926\u001b[0m \u001b[39melif\u001b[39;00m title\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 927\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedResponse(\u001b[39m\"\u001b[39m\u001b[39mYour request couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be processed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    928\u001b[0m \u001b[39melif\u001b[39;00m title\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m temp_ban_titles:\n\u001b[0;32m    929\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mTemporarilyBanned(title\u001b[39m.\u001b[39mtext)\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m: Your request couldn't be processed"
     ]
    }
   ],
   "source": [
    "post_list = []\n",
    "for post in get_posts(FANPAGE_LINK,\n",
    "                    options={\"comments\": True, \"reactions\": True, \"allow_extra_requests\": True},\n",
    "                    extra_info=True, pages=PAGES_NUMBER, cookies=COOKIE_PATH):\n",
    "    print(post)\n",
    "    post_list.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert list of dicts to df\n",
    "\n",
    "Now we can convert the list of dictionaries to a pandas dataframe. We will be using the pandas library to do this. We will also be saving the dataframe to a xlxs or csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe to scrape Facebook post\n",
    "post_df_full = pd.DataFrame(columns=post_list[0].keys(), index=range(len(post_list)), data=post_list)\n",
    "\n",
    "# To df\n",
    "path=FOLDER_PATH + FANPAGE_LINK + \".csv\"\n",
    "post_df_full.to_csv(path, index=False)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_full "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
